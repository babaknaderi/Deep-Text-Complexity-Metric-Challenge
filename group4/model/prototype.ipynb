{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python394jvsc74a57bd04c2c6958f73f61d7b8bffd4193a0d9584c9ef39256921fe54ce430734ddbdc1d",
   "display_name": "Python 3.9.4 64-bit ('QandU': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0      Weitergehende Sicherungsmaßnahmen können eine ...\n",
       "1      In der großen Kasernenanlage im Norden Kiels k...\n",
       "2      Premierminister David Lloyd George honorierte ...\n",
       "3      Der Beitrag der Truppen dieser Dominions währe...\n",
       "4      Eine Balance zwischen verschiedenen Lebensbere...\n",
       "                             ...                        \n",
       "895    Zwischen 1901 und 2010 ist er um ca 1,7 cm pro...\n",
       "896    Aus Furcht vor einem Bürgerkrieg wollte sie – ...\n",
       "897    In den meisten Politikfeldern gilt dafür seit ...\n",
       "898    Aufgrund der Wärmekapazität des Gesteins, und ...\n",
       "899    Die Klinge ist zumeist aus nicht rostfreiem Ko...\n",
       "Name: Sentence, Length: 900, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "# load data and write out sentence and target\n",
    "import pandas as pd\n",
    "\n",
    "loaded_set = pd.read_excel(\"Dataset/\"+\"training.xlsx\")\n",
    "#x_train, x_test,  y_train, y_test = train_test_split(loaded_set['Sentence'], loaded_set['MOS'], test_size=.2)\n",
    "loaded_set['Sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     dativ  akkusativ  genitiv  num_words  letter_count  \\\n",
       "0        1          0        0         31           231   \n",
       "1        1          0        0         11            61   \n",
       "2        1          1        0         28           204   \n",
       "3        1          0        0         24           139   \n",
       "4        1          0        0         36           297   \n",
       "..     ...        ...      ...        ...           ...   \n",
       "895      1          0        0         27           114   \n",
       "896      1          1        0         29           180   \n",
       "897      1          1        0         43           285   \n",
       "898      1          0        1         32           211   \n",
       "899      1          0        0         12            81   \n",
       "\n",
       "     avarange_letter_per_word  longest_word_length  shortest_word_length  \n",
       "0                    7.451613                   22                     2  \n",
       "1                    5.545455                   16                     2  \n",
       "2                    7.285714                   20                     2  \n",
       "3                    5.791667                   11                     3  \n",
       "4                    8.250000                   23                     2  \n",
       "..                        ...                  ...                   ...  \n",
       "895                  4.222222                   10                     2  \n",
       "896                  6.206897                   14                     1  \n",
       "897                  6.627907                   22                     2  \n",
       "898                  6.593750                   15                     3  \n",
       "899                  6.750000                   16                     3  \n",
       "\n",
       "[900 rows x 8 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dativ</th>\n      <th>akkusativ</th>\n      <th>genitiv</th>\n      <th>num_words</th>\n      <th>letter_count</th>\n      <th>avarange_letter_per_word</th>\n      <th>longest_word_length</th>\n      <th>shortest_word_length</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>31</td>\n      <td>231</td>\n      <td>7.451613</td>\n      <td>22</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>11</td>\n      <td>61</td>\n      <td>5.545455</td>\n      <td>16</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>28</td>\n      <td>204</td>\n      <td>7.285714</td>\n      <td>20</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>24</td>\n      <td>139</td>\n      <td>5.791667</td>\n      <td>11</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>36</td>\n      <td>297</td>\n      <td>8.250000</td>\n      <td>23</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>895</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>27</td>\n      <td>114</td>\n      <td>4.222222</td>\n      <td>10</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>896</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>29</td>\n      <td>180</td>\n      <td>6.206897</td>\n      <td>14</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>897</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>43</td>\n      <td>285</td>\n      <td>6.627907</td>\n      <td>22</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>898</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>32</td>\n      <td>211</td>\n      <td>6.593750</td>\n      <td>15</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>899</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>12</td>\n      <td>81</td>\n      <td>6.750000</td>\n      <td>16</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n<p>900 rows × 8 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "letter_count = []\n",
    "avarange_letter_per_word = []\n",
    "num_words = []\n",
    "num_letters_array = []\n",
    "longest_word_length = []\n",
    "shortest_word_length = []\n",
    "genitiv = []\n",
    "akkusativ = []\n",
    "dativ =[]\n",
    "\n",
    "for sen in loaded_set['Sentence']:\n",
    "    current_sen_split = sen.split()\n",
    "    num_words.append(len(current_sen_split))\n",
    "    num_letters = []\n",
    "        \n",
    "    if \"des\" in sen:\n",
    "        genitiv.append(1)\n",
    "    else:\n",
    "        genitiv.append(0)\n",
    "\n",
    "    if \"dem\" in sen:\n",
    "        akkusativ.append(1)\n",
    "    else:\n",
    "        akkusativ.append(0)\n",
    "\n",
    "    if \"den\" in sen:\n",
    "        dativ.append(1)\n",
    "    else:\n",
    "        dativ.append(1)\n",
    "\n",
    "    for y in range(len(current_sen_split)):\n",
    "        current_word = current_sen_split[y]\n",
    "   \n",
    "        num_letters.append(len(current_word))\n",
    "   \n",
    "    current_lettercount = sum(num_letters)\n",
    "    letter_count.append(current_lettercount) \n",
    "    avarange_letter_per_word.append(current_lettercount/len(current_sen_split))\n",
    "    longest_word_length.append(max(num_letters)) \n",
    "    shortest_word_length.append(min(num_letters)) \n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "feature_dict = {\n",
    "    'dativ':dativ, \n",
    "    'akkusativ': akkusativ, \n",
    "    'genitiv': genitiv, \n",
    "    'num_words':num_words,\n",
    "    'letter_count':letter_count,\n",
    "    'avarange_letter_per_word':avarange_letter_per_word,\n",
    "    'longest_word_length':longest_word_length,\n",
    "    'shortest_word_length':shortest_word_length, \n",
    "    }\n",
    "feature_dataframe = pd.DataFrame(data=feature_dict)\n",
    "feature_dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = feature_dataframe\n",
    "y = round(loaded_set['MOS'],2)\n",
    "\n",
    "X_train, X_test,  y_train, y_test = train_test_split(X, y, test_size=.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-84-80b552713886>:4: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  X_train[['num_words', 'longest_word_length', 'shortest_word_length', 'letter_count', 'avarange_letter_per_word']] = scaler.fit_transform(X_train[['num_words', 'longest_word_length', 'shortest_word_length', 'letter_count', 'avarange_letter_per_word']])\nC:\\Anaconda3\\envs\\QandU\\lib\\site-packages\\pandas\\core\\indexing.py:1738: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n<ipython-input-84-80b552713886>:6: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  X_test[['num_words', 'longest_word_length', 'shortest_word_length', 'letter_count', 'avarange_letter_per_word']] = scaler.transform(X_test[['num_words', 'longest_word_length', 'shortest_word_length', 'letter_count', 'avarange_letter_per_word']])\nC:\\Anaconda3\\envs\\QandU\\lib\\site-packages\\pandas\\core\\indexing.py:1738: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "InvalidArgumentError",
     "evalue": "Index out of range using input dim 2; input has only 2 dims [Op:StridedSlice] name: strided_slice/",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-84-80b552713886>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mtensorX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mtensorX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensorX\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m720\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mtensorX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\QandU\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\QandU\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36m_slice_helper\u001b[1;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[0;32m   1038\u001b[0m       \u001b[0mvar_empty\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m       \u001b[0mpacked_begin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpacked_end\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpacked_strides\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvar_empty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m     return strided_slice(\n\u001b[0m\u001b[0;32m   1041\u001b[0m         \u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m         \u001b[0mpacked_begin\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\QandU\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\QandU\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mstrided_slice\u001b[1;34m(input_, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, var, name)\u001b[0m\n\u001b[0;32m   1211\u001b[0m     \u001b[0mstrides\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mones_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1213\u001b[1;33m   op = gen_array_ops.strided_slice(\n\u001b[0m\u001b[0;32m   1214\u001b[0m       \u001b[0minput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m       \u001b[0mbegin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\QandU\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36mstrided_slice\u001b[1;34m(input, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, name)\u001b[0m\n\u001b[0;32m  10502\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10503\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 10504\u001b[1;33m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m  10505\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10506\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\QandU\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   6895\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\" name: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6896\u001b[0m   \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6897\u001b[1;33m   \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6898\u001b[0m   \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6899\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\QandU\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Index out of range using input dim 2; input has only 2 dims [Op:StridedSlice] name: strided_slice/"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train[['num_words', 'longest_word_length', 'shortest_word_length', 'letter_count', 'avarange_letter_per_word']] = scaler.fit_transform(X_train[['num_words', 'longest_word_length', 'shortest_word_length', 'letter_count', 'avarange_letter_per_word']])\n",
    "\n",
    "X_test[['num_words', 'longest_word_length', 'shortest_word_length', 'letter_count', 'avarange_letter_per_word']] = scaler.transform(X_test[['num_words', 'longest_word_length', 'shortest_word_length', 'letter_count', 'avarange_letter_per_word']])\n",
    "\n",
    "\n",
    "tensorX = tf.constant(X_train.values)\n",
    "tensorX = tf.reshape(tensorX [720, 8, 1])\n",
    "tensorX.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_18\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_68 (Dense)             (None, 64)                576       \n_________________________________________________________________\ndense_69 (Dense)             (None, 128)               8320      \n_________________________________________________________________\ndropout_15 (Dropout)         (None, 128)               0         \n_________________________________________________________________\ndense_70 (Dense)             (None, 32)                4128      \n_________________________________________________________________\ndense_71 (Dense)             (None, 1)                 33        \n=================================================================\nTotal params: 13,057\nTrainable params: 13,057\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "n_timesteps =  tensorX.shape[0]\n",
    "n_features =  tensorX.shape[1]\n",
    "model = tf.keras.Sequential([\n",
    "      #tf.keras.layers.GRU(100, input_shape=(n_timesteps, n_features), return_sequences=True, dropout=0.5),\n",
    "      #tf.keras.layers.GRU(8),\n",
    "      tf.keras.layers.InputLayer(8),\n",
    "      #tf.keras.layers.GRU(4),\n",
    "      tf.keras.layers.Dense(64, activation='relu'),\n",
    "      tf.keras.layers.Dense(128, activation='relu'),\n",
    "      tf.keras.layers.Dropout(0.1),\n",
    "      tf.keras.layers.Dense(32, activation='relu'),\n",
    "      tf.keras.layers.Dense(1)\n",
    "  ])\n",
    "\n",
    "model.compile(loss='mean_absolute_error', optimizer=tf.keras.optimizers.Adam(0.001))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "n_input = 1 #how many samples/rows/timesteps to look in the past in order to forecast the next sample\n",
    "n_features= X_train.shape[1] # how many predictors/Xs/features we have to predict y\n",
    "b_size = 12# Number of timeseries samples in each batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 261/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3864\n",
      "Epoch 262/500\n",
      "23/23 [==============================] - 0s 864us/step - loss: 0.3861\n",
      "Epoch 263/500\n",
      "23/23 [==============================] - 0s 773us/step - loss: 0.3805\n",
      "Epoch 264/500\n",
      "23/23 [==============================] - 0s 818us/step - loss: 0.3977\n",
      "Epoch 265/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3894\n",
      "Epoch 266/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3799\n",
      "Epoch 267/500\n",
      "23/23 [==============================] - 0s 909us/step - loss: 0.3682\n",
      "Epoch 268/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3755\n",
      "Epoch 269/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3814\n",
      "Epoch 270/500\n",
      "23/23 [==============================] - 0s 727us/step - loss: 0.3778\n",
      "Epoch 271/500\n",
      "23/23 [==============================] - 0s 773us/step - loss: 0.3816\n",
      "Epoch 272/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3853\n",
      "Epoch 273/500\n",
      "23/23 [==============================] - 0s 727us/step - loss: 0.3898\n",
      "Epoch 274/500\n",
      "23/23 [==============================] - 0s 773us/step - loss: 0.3668\n",
      "Epoch 275/500\n",
      "23/23 [==============================] - 0s 954us/step - loss: 0.3794\n",
      "Epoch 276/500\n",
      "23/23 [==============================] - 0s 727us/step - loss: 0.3709\n",
      "Epoch 277/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3647\n",
      "Epoch 278/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3718\n",
      "Epoch 279/500\n",
      "23/23 [==============================] - 0s 773us/step - loss: 0.3634\n",
      "Epoch 280/500\n",
      "23/23 [==============================] - 0s 864us/step - loss: 0.3732\n",
      "Epoch 281/500\n",
      "23/23 [==============================] - 0s 773us/step - loss: 0.3826\n",
      "Epoch 282/500\n",
      "23/23 [==============================] - 0s 727us/step - loss: 0.3846\n",
      "Epoch 283/500\n",
      "23/23 [==============================] - 0s 818us/step - loss: 0.3786\n",
      "Epoch 284/500\n",
      "23/23 [==============================] - 0s 818us/step - loss: 0.3798\n",
      "Epoch 285/500\n",
      "23/23 [==============================] - 0s 682us/step - loss: 0.3749\n",
      "Epoch 286/500\n",
      "23/23 [==============================] - 0s 864us/step - loss: 0.3838\n",
      "Epoch 287/500\n",
      "23/23 [==============================] - 0s 773us/step - loss: 0.3694\n",
      "Epoch 288/500\n",
      "23/23 [==============================] - 0s 864us/step - loss: 0.3696\n",
      "Epoch 289/500\n",
      "23/23 [==============================] - 0s 909us/step - loss: 0.3696\n",
      "Epoch 290/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3736\n",
      "Epoch 291/500\n",
      "23/23 [==============================] - 0s 727us/step - loss: 0.3649\n",
      "Epoch 292/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3653\n",
      "Epoch 293/500\n",
      "23/23 [==============================] - 0s 864us/step - loss: 0.3661\n",
      "Epoch 294/500\n",
      "23/23 [==============================] - 0s 955us/step - loss: 0.3665\n",
      "Epoch 295/500\n",
      "23/23 [==============================] - 0s 864us/step - loss: 0.3654\n",
      "Epoch 296/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3763\n",
      "Epoch 297/500\n",
      "23/23 [==============================] - 0s 818us/step - loss: 0.3745\n",
      "Epoch 298/500\n",
      "23/23 [==============================] - 0s 818us/step - loss: 0.3549\n",
      "Epoch 299/500\n",
      "23/23 [==============================] - 0s 773us/step - loss: 0.3551\n",
      "Epoch 300/500\n",
      "23/23 [==============================] - 0s 955us/step - loss: 0.3728\n",
      "Epoch 301/500\n",
      "23/23 [==============================] - 0s 909us/step - loss: 0.3772\n",
      "Epoch 302/500\n",
      "23/23 [==============================] - 0s 818us/step - loss: 0.3607\n",
      "Epoch 303/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3751\n",
      "Epoch 304/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3914\n",
      "Epoch 305/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3493\n",
      "Epoch 306/500\n",
      "23/23 [==============================] - 0s 909us/step - loss: 0.3584\n",
      "Epoch 307/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3682\n",
      "Epoch 308/500\n",
      "23/23 [==============================] - 0s 909us/step - loss: 0.3679\n",
      "Epoch 309/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3624\n",
      "Epoch 310/500\n",
      "23/23 [==============================] - 0s 955us/step - loss: 0.3570\n",
      "Epoch 311/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3751\n",
      "Epoch 312/500\n",
      "23/23 [==============================] - 0s 864us/step - loss: 0.3649\n",
      "Epoch 313/500\n",
      "23/23 [==============================] - 0s 955us/step - loss: 0.3634\n",
      "Epoch 314/500\n",
      "23/23 [==============================] - 0s 818us/step - loss: 0.3638\n",
      "Epoch 315/500\n",
      "23/23 [==============================] - 0s 727us/step - loss: 0.3533\n",
      "Epoch 316/500\n",
      "23/23 [==============================] - 0s 773us/step - loss: 0.3582\n",
      "Epoch 317/500\n",
      "23/23 [==============================] - 0s 909us/step - loss: 0.3548\n",
      "Epoch 318/500\n",
      "23/23 [==============================] - 0s 818us/step - loss: 0.3624\n",
      "Epoch 319/500\n",
      "23/23 [==============================] - 0s 773us/step - loss: 0.3633\n",
      "Epoch 320/500\n",
      "23/23 [==============================] - 0s 909us/step - loss: 0.3688\n",
      "Epoch 321/500\n",
      "23/23 [==============================] - 0s 909us/step - loss: 0.3540\n",
      "Epoch 322/500\n",
      "23/23 [==============================] - 0s 864us/step - loss: 0.3565\n",
      "Epoch 323/500\n",
      "23/23 [==============================] - 0s 818us/step - loss: 0.3641\n",
      "Epoch 324/500\n",
      "23/23 [==============================] - 0s 727us/step - loss: 0.3623\n",
      "Epoch 325/500\n",
      "23/23 [==============================] - 0s 773us/step - loss: 0.3469\n",
      "Epoch 326/500\n",
      "23/23 [==============================] - 0s 707us/step - loss: 0.3556\n",
      "Epoch 327/500\n",
      "23/23 [==============================] - 0s 818us/step - loss: 0.3624\n",
      "Epoch 328/500\n",
      "23/23 [==============================] - 0s 955us/step - loss: 0.3637\n",
      "Epoch 329/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3629\n",
      "Epoch 330/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3498\n",
      "Epoch 331/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3515\n",
      "Epoch 332/500\n",
      "23/23 [==============================] - 0s 955us/step - loss: 0.3492\n",
      "Epoch 333/500\n",
      "23/23 [==============================] - 0s 773us/step - loss: 0.3623\n",
      "Epoch 334/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3430\n",
      "Epoch 335/500\n",
      "23/23 [==============================] - 0s 818us/step - loss: 0.3548\n",
      "Epoch 336/500\n",
      "23/23 [==============================] - 0s 864us/step - loss: 0.3525\n",
      "Epoch 337/500\n",
      "23/23 [==============================] - 0s 682us/step - loss: 0.3509\n",
      "Epoch 338/500\n",
      "23/23 [==============================] - 0s 727us/step - loss: 0.3418\n",
      "Epoch 339/500\n",
      "23/23 [==============================] - 0s 727us/step - loss: 0.3396\n",
      "Epoch 340/500\n",
      "23/23 [==============================] - 0s 727us/step - loss: 0.3571\n",
      "Epoch 341/500\n",
      "23/23 [==============================] - 0s 773us/step - loss: 0.3532\n",
      "Epoch 342/500\n",
      "23/23 [==============================] - 0s 773us/step - loss: 0.3460\n",
      "Epoch 343/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3482\n",
      "Epoch 344/500\n",
      "23/23 [==============================] - 0s 773us/step - loss: 0.3525\n",
      "Epoch 345/500\n",
      "23/23 [==============================] - 0s 864us/step - loss: 0.3391\n",
      "Epoch 346/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3554\n",
      "Epoch 347/500\n",
      "23/23 [==============================] - 0s 909us/step - loss: 0.3410\n",
      "Epoch 348/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3436\n",
      "Epoch 349/500\n",
      "23/23 [==============================] - 0s 864us/step - loss: 0.3508\n",
      "Epoch 350/500\n",
      "23/23 [==============================] - 0s 773us/step - loss: 0.3458\n",
      "Epoch 351/500\n",
      "23/23 [==============================] - 0s 773us/step - loss: 0.3421\n",
      "Epoch 352/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3524\n",
      "Epoch 353/500\n",
      "23/23 [==============================] - 0s 818us/step - loss: 0.3486\n",
      "Epoch 354/500\n",
      "23/23 [==============================] - 0s 955us/step - loss: 0.3553\n",
      "Epoch 355/500\n",
      "23/23 [==============================] - 0s 864us/step - loss: 0.3399\n",
      "Epoch 356/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3349\n",
      "Epoch 357/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3520\n",
      "Epoch 358/500\n",
      "23/23 [==============================] - 0s 818us/step - loss: 0.3443\n",
      "Epoch 359/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3653\n",
      "Epoch 360/500\n",
      "23/23 [==============================] - 0s 818us/step - loss: 0.3531\n",
      "Epoch 361/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3474\n",
      "Epoch 362/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3349\n",
      "Epoch 363/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3489\n",
      "Epoch 364/500\n",
      "23/23 [==============================] - 0s 773us/step - loss: 0.3488\n",
      "Epoch 365/500\n",
      "23/23 [==============================] - 0s 864us/step - loss: 0.3417\n",
      "Epoch 366/500\n",
      "23/23 [==============================] - 0s 955us/step - loss: 0.3431\n",
      "Epoch 367/500\n",
      "23/23 [==============================] - 0s 864us/step - loss: 0.3466\n",
      "Epoch 368/500\n",
      "23/23 [==============================] - 0s 773us/step - loss: 0.3386\n",
      "Epoch 369/500\n",
      "23/23 [==============================] - 0s 818us/step - loss: 0.3435\n",
      "Epoch 370/500\n",
      "23/23 [==============================] - 0s 682us/step - loss: 0.3324\n",
      "Epoch 371/500\n",
      "23/23 [==============================] - 0s 773us/step - loss: 0.3355\n",
      "Epoch 372/500\n",
      "23/23 [==============================] - 0s 728us/step - loss: 0.3395\n",
      "Epoch 373/500\n",
      "23/23 [==============================] - 0s 773us/step - loss: 0.3409\n",
      "Epoch 374/500\n",
      "23/23 [==============================] - 0s 773us/step - loss: 0.3593\n",
      "Epoch 375/500\n",
      "23/23 [==============================] - 0s 773us/step - loss: 0.3362\n",
      "Epoch 376/500\n",
      "23/23 [==============================] - 0s 727us/step - loss: 0.3349\n",
      "Epoch 377/500\n",
      "23/23 [==============================] - 0s 864us/step - loss: 0.3440\n",
      "Epoch 378/500\n",
      "23/23 [==============================] - 0s 818us/step - loss: 0.3434\n",
      "Epoch 379/500\n",
      "23/23 [==============================] - 0s 818us/step - loss: 0.3441\n",
      "Epoch 380/500\n",
      "23/23 [==============================] - 0s 909us/step - loss: 0.3361\n",
      "Epoch 381/500\n",
      "23/23 [==============================] - 0s 998us/step - loss: 0.3515\n",
      "Epoch 382/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3411\n",
      "Epoch 383/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3376\n",
      "Epoch 384/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3340\n",
      "Epoch 385/500\n",
      "23/23 [==============================] - 0s 818us/step - loss: 0.3464\n",
      "Epoch 386/500\n",
      "23/23 [==============================] - 0s 909us/step - loss: 0.3559\n",
      "Epoch 387/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3362\n",
      "Epoch 388/500\n",
      "23/23 [==============================] - 0s 909us/step - loss: 0.3318\n",
      "Epoch 389/500\n",
      "23/23 [==============================] - 0s 955us/step - loss: 0.3399\n",
      "Epoch 390/500\n",
      "23/23 [==============================] - 0s 864us/step - loss: 0.3412\n",
      "Epoch 391/500\n",
      "23/23 [==============================] - 0s 818us/step - loss: 0.3358\n",
      "Epoch 392/500\n",
      "23/23 [==============================] - 0s 773us/step - loss: 0.3246\n",
      "Epoch 393/500\n",
      "23/23 [==============================] - 0s 909us/step - loss: 0.3329\n",
      "Epoch 394/500\n",
      "23/23 [==============================] - 0s 819us/step - loss: 0.3287\n",
      "Epoch 395/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3504\n",
      "Epoch 396/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3211\n",
      "Epoch 397/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3366\n",
      "Epoch 398/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3330\n",
      "Epoch 399/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3387\n",
      "Epoch 400/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3362\n",
      "Epoch 401/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3373\n",
      "Epoch 402/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3297\n",
      "Epoch 403/500\n",
      "23/23 [==============================] - 0s 955us/step - loss: 0.3275\n",
      "Epoch 404/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3286\n",
      "Epoch 405/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3296\n",
      "Epoch 406/500\n",
      "23/23 [==============================] - 0s 818us/step - loss: 0.3315\n",
      "Epoch 407/500\n",
      "23/23 [==============================] - 0s 955us/step - loss: 0.3335\n",
      "Epoch 408/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3321\n",
      "Epoch 409/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3324\n",
      "Epoch 410/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3372\n",
      "Epoch 411/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3309\n",
      "Epoch 412/500\n",
      "23/23 [==============================] - 0s 864us/step - loss: 0.3230\n",
      "Epoch 413/500\n",
      "23/23 [==============================] - 0s 818us/step - loss: 0.3217\n",
      "Epoch 414/500\n",
      "23/23 [==============================] - 0s 682us/step - loss: 0.3301\n",
      "Epoch 415/500\n",
      "23/23 [==============================] - 0s 727us/step - loss: 0.3318\n",
      "Epoch 416/500\n",
      "23/23 [==============================] - 0s 728us/step - loss: 0.3331\n",
      "Epoch 417/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3390\n",
      "Epoch 418/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3321\n",
      "Epoch 419/500\n",
      "23/23 [==============================] - 0s 909us/step - loss: 0.3211\n",
      "Epoch 420/500\n",
      "23/23 [==============================] - 0s 909us/step - loss: 0.3318\n",
      "Epoch 421/500\n",
      "23/23 [==============================] - 0s 955us/step - loss: 0.3362\n",
      "Epoch 422/500\n",
      "23/23 [==============================] - 0s 773us/step - loss: 0.3303\n",
      "Epoch 423/500\n",
      "23/23 [==============================] - 0s 728us/step - loss: 0.3266\n",
      "Epoch 424/500\n",
      "23/23 [==============================] - 0s 909us/step - loss: 0.3221\n",
      "Epoch 425/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3211\n",
      "Epoch 426/500\n",
      "23/23 [==============================] - 0s 727us/step - loss: 0.3261\n",
      "Epoch 427/500\n",
      "23/23 [==============================] - 0s 682us/step - loss: 0.3270\n",
      "Epoch 428/500\n",
      "23/23 [==============================] - 0s 682us/step - loss: 0.3326\n",
      "Epoch 429/500\n",
      "23/23 [==============================] - 0s 727us/step - loss: 0.3194\n",
      "Epoch 430/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3219\n",
      "Epoch 431/500\n",
      "23/23 [==============================] - 0s 727us/step - loss: 0.3306\n",
      "Epoch 432/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3354\n",
      "Epoch 433/500\n",
      "23/23 [==============================] - 0s 818us/step - loss: 0.3135\n",
      "Epoch 434/500\n",
      "23/23 [==============================] - 0s 718us/step - loss: 0.3236\n",
      "Epoch 435/500\n",
      "23/23 [==============================] - 0s 773us/step - loss: 0.3220\n",
      "Epoch 436/500\n",
      "23/23 [==============================] - 0s 864us/step - loss: 0.3228\n",
      "Epoch 437/500\n",
      "23/23 [==============================] - 0s 727us/step - loss: 0.3317\n",
      "Epoch 438/500\n",
      "23/23 [==============================] - 0s 727us/step - loss: 0.3187\n",
      "Epoch 439/500\n",
      "23/23 [==============================] - 0s 818us/step - loss: 0.3227\n",
      "Epoch 440/500\n",
      "23/23 [==============================] - 0s 773us/step - loss: 0.3188\n",
      "Epoch 441/500\n",
      "23/23 [==============================] - 0s 864us/step - loss: 0.3130\n",
      "Epoch 442/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3217\n",
      "Epoch 443/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3215\n",
      "Epoch 444/500\n",
      "23/23 [==============================] - 0s 818us/step - loss: 0.3221\n",
      "Epoch 445/500\n",
      "23/23 [==============================] - 0s 909us/step - loss: 0.3176\n",
      "Epoch 446/500\n",
      "23/23 [==============================] - 0s 909us/step - loss: 0.3211\n",
      "Epoch 447/500\n",
      "23/23 [==============================] - 0s 955us/step - loss: 0.3177\n",
      "Epoch 448/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3171\n",
      "Epoch 449/500\n",
      "23/23 [==============================] - 0s 909us/step - loss: 0.3210\n",
      "Epoch 450/500\n",
      "23/23 [==============================] - 0s 909us/step - loss: 0.3314\n",
      "Epoch 451/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3173\n",
      "Epoch 452/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3263\n",
      "Epoch 453/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3281\n",
      "Epoch 454/500\n",
      "23/23 [==============================] - 0s 773us/step - loss: 0.3262\n",
      "Epoch 455/500\n",
      "23/23 [==============================] - 0s 727us/step - loss: 0.3125\n",
      "Epoch 456/500\n",
      "23/23 [==============================] - 0s 727us/step - loss: 0.3104\n",
      "Epoch 457/500\n",
      "23/23 [==============================] - 0s 773us/step - loss: 0.3332\n",
      "Epoch 458/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3302\n",
      "Epoch 459/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3145\n",
      "Epoch 460/500\n",
      "23/23 [==============================] - 0s 773us/step - loss: 0.3205\n",
      "Epoch 461/500\n",
      "23/23 [==============================] - 0s 955us/step - loss: 0.3187\n",
      "Epoch 462/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3087\n",
      "Epoch 463/500\n",
      "23/23 [==============================] - 0s 818us/step - loss: 0.3150\n",
      "Epoch 464/500\n",
      "23/23 [==============================] - 0s 773us/step - loss: 0.3233\n",
      "Epoch 465/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3176\n",
      "Epoch 466/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3189\n",
      "Epoch 467/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3162\n",
      "Epoch 468/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3179\n",
      "Epoch 469/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3122\n",
      "Epoch 470/500\n",
      "23/23 [==============================] - 0s 773us/step - loss: 0.3251\n",
      "Epoch 471/500\n",
      "23/23 [==============================] - 0s 727us/step - loss: 0.3156\n",
      "Epoch 472/500\n",
      "23/23 [==============================] - 0s 727us/step - loss: 0.3109\n",
      "Epoch 473/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3172\n",
      "Epoch 474/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3196\n",
      "Epoch 475/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3088\n",
      "Epoch 476/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3240\n",
      "Epoch 477/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3167\n",
      "Epoch 478/500\n",
      "23/23 [==============================] - 0s 909us/step - loss: 0.3149\n",
      "Epoch 479/500\n",
      "23/23 [==============================] - 0s 864us/step - loss: 0.3003\n",
      "Epoch 480/500\n",
      "23/23 [==============================] - 0s 727us/step - loss: 0.3182\n",
      "Epoch 481/500\n",
      "23/23 [==============================] - 0s 955us/step - loss: 0.3008\n",
      "Epoch 482/500\n",
      "23/23 [==============================] - 0s 955us/step - loss: 0.3176\n",
      "Epoch 483/500\n",
      "23/23 [==============================] - 0s 955us/step - loss: 0.3156\n",
      "Epoch 484/500\n",
      "23/23 [==============================] - 0s 727us/step - loss: 0.3069\n",
      "Epoch 485/500\n",
      "23/23 [==============================] - 0s 727us/step - loss: 0.3055\n",
      "Epoch 486/500\n",
      "23/23 [==============================] - 0s 864us/step - loss: 0.3181\n",
      "Epoch 487/500\n",
      "23/23 [==============================] - 0s 682us/step - loss: 0.3099\n",
      "Epoch 488/500\n",
      "23/23 [==============================] - 0s 727us/step - loss: 0.3294\n",
      "Epoch 489/500\n",
      "23/23 [==============================] - 0s 727us/step - loss: 0.3332\n",
      "Epoch 490/500\n",
      "23/23 [==============================] - 0s 682us/step - loss: 0.3247\n",
      "Epoch 491/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3132\n",
      "Epoch 492/500\n",
      "23/23 [==============================] - 0s 909us/step - loss: 0.3134\n",
      "Epoch 493/500\n",
      "23/23 [==============================] - 0s 910us/step - loss: 0.3094\n",
      "Epoch 494/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3265\n",
      "Epoch 495/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3198\n",
      "Epoch 496/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3111\n",
      "Epoch 497/500\n",
      "23/23 [==============================] - 0s 773us/step - loss: 0.3048\n",
      "Epoch 498/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3227\n",
      "Epoch 499/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3212\n",
      "Epoch 500/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3139\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e2c50a7280>"
      ]
     },
     "metadata": {},
     "execution_count": 89
    }
   ],
   "source": [
    "model.fit(tensorX ,y_train,epochs=500, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[2.68],\n",
       "       [1.87],\n",
       "       [4.37],\n",
       "       [2.68],\n",
       "       [4.13],\n",
       "       [4.22],\n",
       "       [3.8 ],\n",
       "       [2.64],\n",
       "       [1.01],\n",
       "       [2.32],\n",
       "       [2.46],\n",
       "       [3.84],\n",
       "       [3.04],\n",
       "       [1.92],\n",
       "       [3.12],\n",
       "       [3.82],\n",
       "       [1.85],\n",
       "       [2.92],\n",
       "       [3.9 ],\n",
       "       [5.21],\n",
       "       [3.3 ],\n",
       "       [2.16],\n",
       "       [2.41],\n",
       "       [3.74],\n",
       "       [3.96],\n",
       "       [4.2 ],\n",
       "       [3.42],\n",
       "       [3.04],\n",
       "       [3.41],\n",
       "       [4.06],\n",
       "       [4.63],\n",
       "       [3.36],\n",
       "       [1.6 ],\n",
       "       [4.47],\n",
       "       [3.95],\n",
       "       [1.74],\n",
       "       [3.79],\n",
       "       [1.55],\n",
       "       [2.48],\n",
       "       [4.86],\n",
       "       [1.75],\n",
       "       [1.74],\n",
       "       [3.81],\n",
       "       [2.8 ],\n",
       "       [4.19],\n",
       "       [2.06],\n",
       "       [3.1 ],\n",
       "       [4.32],\n",
       "       [2.12],\n",
       "       [2.54],\n",
       "       [2.18],\n",
       "       [4.78],\n",
       "       [3.3 ],\n",
       "       [2.63],\n",
       "       [4.77],\n",
       "       [2.53],\n",
       "       [3.38],\n",
       "       [2.04],\n",
       "       [2.29],\n",
       "       [3.05],\n",
       "       [3.01],\n",
       "       [4.03],\n",
       "       [1.24],\n",
       "       [2.44],\n",
       "       [3.4 ],\n",
       "       [4.05],\n",
       "       [4.35],\n",
       "       [1.94],\n",
       "       [1.57],\n",
       "       [2.42],\n",
       "       [2.92],\n",
       "       [3.89],\n",
       "       [3.23],\n",
       "       [2.87],\n",
       "       [2.36],\n",
       "       [3.89],\n",
       "       [1.  ],\n",
       "       [1.03],\n",
       "       [4.49],\n",
       "       [1.62],\n",
       "       [2.72],\n",
       "       [4.08],\n",
       "       [4.23],\n",
       "       [4.61],\n",
       "       [1.41],\n",
       "       [3.66],\n",
       "       [3.39],\n",
       "       [2.95],\n",
       "       [4.31],\n",
       "       [1.97],\n",
       "       [2.62],\n",
       "       [2.83],\n",
       "       [2.01],\n",
       "       [2.27],\n",
       "       [2.56],\n",
       "       [3.39],\n",
       "       [1.4 ],\n",
       "       [2.76],\n",
       "       [2.4 ],\n",
       "       [1.12],\n",
       "       [2.96],\n",
       "       [1.49],\n",
       "       [3.15],\n",
       "       [3.24],\n",
       "       [3.13],\n",
       "       [2.62],\n",
       "       [2.44],\n",
       "       [2.77],\n",
       "       [5.32],\n",
       "       [1.84],\n",
       "       [1.18],\n",
       "       [4.08],\n",
       "       [2.57],\n",
       "       [3.02],\n",
       "       [1.76],\n",
       "       [3.23],\n",
       "       [2.1 ],\n",
       "       [3.17],\n",
       "       [1.28],\n",
       "       [2.68],\n",
       "       [2.19],\n",
       "       [3.7 ],\n",
       "       [3.63],\n",
       "       [1.64],\n",
       "       [3.98],\n",
       "       [1.86],\n",
       "       [3.63],\n",
       "       [4.44],\n",
       "       [4.02],\n",
       "       [4.15],\n",
       "       [0.99],\n",
       "       [3.73],\n",
       "       [3.19],\n",
       "       [2.56],\n",
       "       [1.15],\n",
       "       [1.93],\n",
       "       [2.79],\n",
       "       [4.75],\n",
       "       [2.7 ],\n",
       "       [3.18],\n",
       "       [3.6 ],\n",
       "       [1.01],\n",
       "       [3.85],\n",
       "       [4.52],\n",
       "       [3.27],\n",
       "       [1.67],\n",
       "       [2.86],\n",
       "       [3.33],\n",
       "       [2.71],\n",
       "       [3.07],\n",
       "       [4.66],\n",
       "       [3.18],\n",
       "       [1.07],\n",
       "       [1.46],\n",
       "       [2.21],\n",
       "       [2.06],\n",
       "       [1.49],\n",
       "       [3.79],\n",
       "       [0.99],\n",
       "       [2.74],\n",
       "       [1.66],\n",
       "       [3.26],\n",
       "       [3.69],\n",
       "       [5.31],\n",
       "       [1.99],\n",
       "       [3.26],\n",
       "       [1.82],\n",
       "       [4.59],\n",
       "       [4.15],\n",
       "       [2.53],\n",
       "       [3.51],\n",
       "       [3.17],\n",
       "       [2.33],\n",
       "       [3.22],\n",
       "       [4.61],\n",
       "       [3.43],\n",
       "       [4.61],\n",
       "       [3.51],\n",
       "       [3.3 ],\n",
       "       [3.48]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 90
    }
   ],
   "source": [
    "import numpy as np\n",
    "pred = model.predict(X_test)\n",
    "rounded_pred = np.around(pred, decimals=2)\n",
    "rounded_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.71567434], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 91
    }
   ],
   "source": [
    "def pearsonr(x, y):\n",
    "  # Assume len(x) == len(y)\n",
    "  n = len(x)\n",
    "  sum_x = float(sum(x))\n",
    "  sum_y = float(sum(y))\n",
    "  sum_x_sq = sum(xi*xi for xi in x)\n",
    "  sum_y_sq = sum(yi*yi for yi in y)\n",
    "  psum = sum(xi*yi for xi, yi in zip(x, y))\n",
    "  num = psum - (sum_x * sum_y/n)\n",
    "  den = pow((sum_x_sq - pow(sum_x, 2) / n) * (sum_y_sq - pow(sum_y, 2) / n), 0.5)\n",
    "  if den == 0: return 0\n",
    "  return num / den\n",
    "pearsonr(rounded_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}